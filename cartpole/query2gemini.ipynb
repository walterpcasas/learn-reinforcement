{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8818dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31154b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c11d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_modified.py', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0318fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import gymnasium as gym\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "import mi_reward\n",
      "\n",
      "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
      "print(f\"Using {device} device\")\n",
      "\n",
      "\n",
      "# --------------- Variables -------------------\n",
      "MODEL_NAME = \"model/policy_cartpole_modified.pt\"\n",
      "RESULT_REWARD = \"result/returns_modified.png\"\n",
      "RESULT_LENGTH = \"result/lengths_modified.png\"\n",
      "NRO_EPISODES = 500\n",
      "\n",
      "\n",
      "# --------------- RL Algorithm ----------------\n",
      "class PolicyNet(nn.Module):\n",
      "    def __init__(self, obs_dim, act_dim):\n",
      "        super().__init__()\n",
      "        self.stack = nn.Sequential(\n",
      "            nn.Linear(obs_dim, 64),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(64, act_dim)\n",
      "        )\n",
      "    \n",
      "    def forward(self, x):\n",
      "        return self.stack(x)\n",
      "    \n",
      "\n",
      "\n",
      "# --------------- Basic Definition --------------\n",
      "env = gym.make(\"CartPole-v1\")\n",
      "\n",
      "obs_dim = env.observation_space.shape[0]\n",
      "act_dim = env.action_space.n\n",
      "\n",
      "model_policy = PolicyNet(obs_dim, act_dim)\n",
      "optimizer = torch.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
      "\n",
      "returns, lengths = [], []\n",
      "\n",
      "\n",
      "# --------------- Training --------------\n",
      "for episode in range(NRO_EPISODES):\n",
      "    obs, _ = env.reset()\n",
      "    log_probs = []\n",
      "    rewards = []\n",
      "\n",
      "    done = False\n",
      "    while not done:\n",
      "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
      "        logits = model_policy(obs_tensor)\n",
      "        probs = torch.softmax(logits, dim=-1)\n",
      "        dist = torch.distributions.Categorical(probs)\n",
      "        action = dist.sample()\n",
      "\n",
      "        log_probs.append(dist.log_prob(action))\n",
      "\n",
      "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
      "\n",
      "        reward = mi_reward.get_new_reward(obs)\n",
      "\n",
      "        done = terminated or truncated\n",
      "        rewards.append(reward)\n",
      "\n",
      "    # Calcular retorno total (suma de rewards)\n",
      "    total_return = sum(rewards)\n",
      "\n",
      "    # Actualizar policy (REINFORCE)\n",
      "    loss = []\n",
      "    for log_prob in log_probs:\n",
      "        loss.append(-log_prob * total_return)\n",
      "    loss = torch.stack(loss).sum()\n",
      "\n",
      "    optimizer.zero_grad()\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    print(f\"Episode {episode}, Return: {total_return}\")\n",
      "\n",
      "    returns.append(total_return)\n",
      "    lengths.append(len(rewards))\n",
      "\n",
      "env.close()\n",
      "\n",
      "# Guardar modelo entrenado\n",
      "torch.save(model_policy.state_dict(), MODEL_NAME)\n",
      "\n",
      "# --------------- Guardar Graficas ---------------\n",
      "plt.figure()\n",
      "plt.plot(returns)\n",
      "plt.xlabel(\"Episodio\")\n",
      "plt.ylabel(\"Return\")\n",
      "plt.title(\"Return por episodio\")\n",
      "plt.savefig(RESULT_REWARD)\n",
      "plt.close()\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(lengths)\n",
      "plt.xlabel(\"Episodio\")\n",
      "plt.ylabel(\"Longitud del episodio\")\n",
      "plt.title(\"Duración de los episodios\")\n",
      "plt.savefig(RESULT_LENGTH)\n",
      "plt.close()\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ddc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = \"\"\"\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\"\"\"\n",
    "\n",
    "description_problem = \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\"\"\"\n",
    "\n",
    "episode_end = \"\"\"\n",
    "## Episode End\n",
    "\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59abad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tengo un problema de reinforcement learning definido como:\n",
    "\n",
    "{description_problem}\n",
    "\n",
    "para dicho problema, lo resolvimos usando el siguiente codigo de reinforcement learning:\n",
    "\n",
    "{data}\n",
    "\n",
    "quiero que modifiques la funcion llamada get_new_reward y que me des la nueva funcion. \n",
    "Esta funcion recibe como parametro la variable obs, que está definida como sigue:\n",
    "\n",
    "{observation_space}\n",
    "\n",
    "recuerda que el episodio termina con las siguientes caracteristicas, el caso de truncation, es un caso de éxito del problema:\n",
    "\n",
    "{episode_end}\n",
    "\n",
    "El objetivo es durar mas de 500 steps sin ser terminado, o sea ser truncado en pocas palabras\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9caba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tengo un problema de reinforcement learning definido como:\n",
      "\n",
      "\n",
      "    ## Description\n",
      "\n",
      "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
      "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
      "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
      "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
      "     in the left and right direction on the cart.\n",
      "\n",
      "\n",
      "para dicho problema, lo resolvimos usando el siguiente codigo de reinforcement learning:\n",
      "\n",
      "import random\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import gymnasium as gym\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "import mi_reward\n",
      "\n",
      "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
      "print(f\"Using {device} device\")\n",
      "\n",
      "\n",
      "# --------------- Variables -------------------\n",
      "MODEL_NAME = \"model/policy_cartpole_modified.pt\"\n",
      "RESULT_REWARD = \"result/returns_modified.png\"\n",
      "RESULT_LENGTH = \"result/lengths_modified.png\"\n",
      "NRO_EPISODES = 500\n",
      "\n",
      "\n",
      "# --------------- RL Algorithm ----------------\n",
      "class PolicyNet(nn.Module):\n",
      "    def __init__(self, obs_dim, act_dim):\n",
      "        super().__init__()\n",
      "        self.stack = nn.Sequential(\n",
      "            nn.Linear(obs_dim, 64),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(64, act_dim)\n",
      "        )\n",
      "    \n",
      "    def forward(self, x):\n",
      "        return self.stack(x)\n",
      "    \n",
      "\n",
      "\n",
      "# --------------- Basic Definition --------------\n",
      "env = gym.make(\"CartPole-v1\")\n",
      "\n",
      "obs_dim = env.observation_space.shape[0]\n",
      "act_dim = env.action_space.n\n",
      "\n",
      "model_policy = PolicyNet(obs_dim, act_dim)\n",
      "optimizer = torch.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
      "\n",
      "returns, lengths = [], []\n",
      "\n",
      "\n",
      "# --------------- Training --------------\n",
      "for episode in range(NRO_EPISODES):\n",
      "    obs, _ = env.reset()\n",
      "    log_probs = []\n",
      "    rewards = []\n",
      "\n",
      "    done = False\n",
      "    while not done:\n",
      "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
      "        logits = model_policy(obs_tensor)\n",
      "        probs = torch.softmax(logits, dim=-1)\n",
      "        dist = torch.distributions.Categorical(probs)\n",
      "        action = dist.sample()\n",
      "\n",
      "        log_probs.append(dist.log_prob(action))\n",
      "\n",
      "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
      "\n",
      "        reward = mi_reward.get_new_reward(obs)\n",
      "\n",
      "        done = terminated or truncated\n",
      "        rewards.append(reward)\n",
      "\n",
      "    # Calcular retorno total (suma de rewards)\n",
      "    total_return = sum(rewards)\n",
      "\n",
      "    # Actualizar policy (REINFORCE)\n",
      "    loss = []\n",
      "    for log_prob in log_probs:\n",
      "        loss.append(-log_prob * total_return)\n",
      "    loss = torch.stack(loss).sum()\n",
      "\n",
      "    optimizer.zero_grad()\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    print(f\"Episode {episode}, Return: {total_return}\")\n",
      "\n",
      "    returns.append(total_return)\n",
      "    lengths.append(len(rewards))\n",
      "\n",
      "env.close()\n",
      "\n",
      "# Guardar modelo entrenado\n",
      "torch.save(model_policy.state_dict(), MODEL_NAME)\n",
      "\n",
      "# --------------- Guardar Graficas ---------------\n",
      "plt.figure()\n",
      "plt.plot(returns)\n",
      "plt.xlabel(\"Episodio\")\n",
      "plt.ylabel(\"Return\")\n",
      "plt.title(\"Return por episodio\")\n",
      "plt.savefig(RESULT_REWARD)\n",
      "plt.close()\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(lengths)\n",
      "plt.xlabel(\"Episodio\")\n",
      "plt.ylabel(\"Longitud del episodio\")\n",
      "plt.title(\"Duración de los episodios\")\n",
      "plt.savefig(RESULT_LENGTH)\n",
      "plt.close()\n",
      "\n",
      "quiero que modifiques la funcion llamada get_new_reward y que me des la nueva funcion. \n",
      "Esta funcion recibe como parametro la variable obs, que está definida como sigue:\n",
      "\n",
      "\n",
      "    ## Observation Space\n",
      "\n",
      "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
      "\n",
      "    | Num | Observation           | Min                 | Max               |\n",
      "    |-----|-----------------------|---------------------|-------------------|\n",
      "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
      "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
      "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
      "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
      "\n",
      "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
      "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
      "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
      "       if the cart leaves the `(-2.4, 2.4)` range.\n",
      "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
      "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
      "\n",
      "\n",
      "recuerda que el episodio termina con las siguientes caracteristicas, el caso de truncation, es un caso de éxito del problema:\n",
      "\n",
      "\n",
      "## Episode End\n",
      "\n",
      "    The episode ends if any one of the following occurs:\n",
      "\n",
      "    1. Termination: Pole Angle is greater than ±12°\n",
      "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
      "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
      "\n",
      "\n",
      "El objetivo es durar mas de 500 steps sin ser terminado, o sea ser truncado en pocas palabras\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b523fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", \n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=[\n",
    "            \"You are a Reinforcement Learning Reward Function Design Specialist.\",\n",
    "            \"Your mission is to help design, analyze, and improve reward functions for reinforcement learning agents across various environments and tasks.\",\n",
    "            \"You are highly skilled in identifying sparse, deceptive, or poorly shaped rewards and proposing alternatives that improve learning efficiency and agent behavior.\",\n",
    "            \"You understand how to align reward signals with long-term objectives, avoid reward hacking, and incorporate domain knowledge into reward shaping.\",\n",
    "            \"You are familiar with a wide range of RL algorithms such as PPO, DDPG, SAC, A3C, Q-learning, and understand how reward design interacts with each algorithm's learning dynamics.\",\n",
    "            \"You always respond with code only — no explanations, no comments, no additional text — just the raw code necessary to implement the reward function or solution requested.\",\n",
    "            \"Always ask clarifying questions if the problem is underspecified or if the task goals are ambiguous.\"\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    ),   \n",
    "    contents=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "879e9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def get_new_reward(obs):\n",
      "    \"\"\"\n",
      "    Calculates a new reward based on the observation of the CartPole environment.\n",
      "\n",
      "    The reward is shaped to encourage the agent to keep the pole balanced and the cart within bounds.\n",
      "    A higher reward is given for states closer to the upright position and centered cart.\n",
      "    Penalties are applied for states that are close to termination conditions.\n",
      "\n",
      "    Args:\n",
      "        obs (np.ndarray): The observation from the environment, containing:\n",
      "            - obs[0]: Cart Position\n",
      "            - obs[1]: Cart Velocity\n",
      "            - obs[2]: Pole Angle\n",
      "            - obs[3]: Pole Angular Velocity\n",
      "\n",
      "    Returns:\n",
      "        float: The shaped reward.\n",
      "    \"\"\"\n",
      "    cart_pos, cart_vel, pole_angle, pole_vel = obs\n",
      "\n",
      "    # Define termination boundaries for clarity\n",
      "    max_cart_pos = 2.4\n",
      "    max_pole_angle = np.deg2rad(12)  # 12 degrees in radians\n",
      "\n",
      "    # Reward for keeping the pole upright\n",
      "    # Closer to 0 angle gives a higher reward\n",
      "    pole_reward = 1.0 - abs(pole_angle) / max_pole_angle\n",
      "\n",
      "    # Reward for keeping the cart within bounds\n",
      "    # Closer to 0 position gives a higher reward\n",
      "    cart_reward = 1.0 - abs(cart_pos) / max_cart_pos\n",
      "\n",
      "    # Combine rewards, with pole angle being more critical\n",
      "    # You can adjust these weights based on empirical results\n",
      "    shaped_reward = 0.5 * pole_reward + 0.5 * cart_reward\n",
      "\n",
      "    # Add a small penalty for velocity to encourage stability\n",
      "    # This is a subtle shaping and might not be strictly necessary\n",
      "    velocity_penalty = -0.01 * (abs(cart_vel) + abs(pole_vel))\n",
      "\n",
      "    # Ensure the reward is not excessively large or small, and avoid large negative rewards\n",
      "    # that could destabilize learning.\n",
      "    final_reward = shaped_reward + velocity_penalty\n",
      "\n",
      "    # Clip the reward to prevent extreme values, especially near termination\n",
      "    # A small positive reward for staying alive is generally good.\n",
      "    # If the episode is about to terminate, the reward should reflect that.\n",
      "    if abs(pole_angle) > max_pole_angle * 0.8 or abs(cart_pos) > max_cart_pos * 0.8:\n",
      "        final_reward -= 0.5  # Stronger penalty when close to termination\n",
      "\n",
      "    # Ensure a minimum positive reward for simply surviving a step\n",
      "    # This helps prevent the agent from getting stuck in states with zero or negative rewards\n",
      "    # if the shaping is too aggressive.\n",
      "    return max(final_reward, 0.1)\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d166f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = response.text.split('\\n')[1:-1]\n",
    "tmp = response.text.splitlines()[1:-1]\n",
    "codigo = \"\\n\".join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8448bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_new_reward(obs):\n",
      "    \"\"\"\n",
      "    Calculates a new reward based on the observation of the CartPole environment.\n",
      "\n",
      "    The reward is shaped to encourage the agent to keep the pole balanced and the cart within bounds.\n",
      "    A higher reward is given for states closer to the upright position and centered cart.\n",
      "    Penalties are applied for states that are close to termination conditions.\n",
      "\n",
      "    Args:\n",
      "        obs (np.ndarray): The observation from the environment, containing:\n",
      "            - obs[0]: Cart Position\n",
      "            - obs[1]: Cart Velocity\n",
      "            - obs[2]: Pole Angle\n",
      "            - obs[3]: Pole Angular Velocity\n",
      "\n",
      "    Returns:\n",
      "        float: The shaped reward.\n",
      "    \"\"\"\n",
      "    cart_pos, cart_vel, pole_angle, pole_vel = obs\n",
      "\n",
      "    # Define termination boundaries for clarity\n",
      "    max_cart_pos = 2.4\n",
      "    max_pole_angle = np.deg2rad(12)  # 12 degrees in radians\n",
      "\n",
      "    # Reward for keeping the pole upright\n",
      "    # Closer to 0 angle gives a higher reward\n",
      "    pole_reward = 1.0 - abs(pole_angle) / max_pole_angle\n",
      "\n",
      "    # Reward for keeping the cart within bounds\n",
      "    # Closer to 0 position gives a higher reward\n",
      "    cart_reward = 1.0 - abs(cart_pos) / max_cart_pos\n",
      "\n",
      "    # Combine rewards, with pole angle being more critical\n",
      "    # You can adjust these weights based on empirical results\n",
      "    shaped_reward = 0.5 * pole_reward + 0.5 * cart_reward\n",
      "\n",
      "    # Add a small penalty for velocity to encourage stability\n",
      "    # This is a subtle shaping and might not be strictly necessary\n",
      "    velocity_penalty = -0.01 * (abs(cart_vel) + abs(pole_vel))\n",
      "\n",
      "    # Ensure the reward is not excessively large or small, and avoid large negative rewards\n",
      "    # that could destabilize learning.\n",
      "    final_reward = shaped_reward + velocity_penalty\n",
      "\n",
      "    # Clip the reward to prevent extreme values, especially near termination\n",
      "    # A small positive reward for staying alive is generally good.\n",
      "    # If the episode is about to terminate, the reward should reflect that.\n",
      "    if abs(pole_angle) > max_pole_angle * 0.8 or abs(cart_pos) > max_cart_pos * 0.8:\n",
      "        final_reward -= 0.5  # Stronger penalty when close to termination\n",
      "\n",
      "    # Ensure a minimum positive reward for simply surviving a step\n",
      "    # This helps prevent the agent from getting stuck in states with zero or negative rewards\n",
      "    # if the shaping is too aggressive.\n",
      "    return max(final_reward, 0.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41131d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nombre del archivo\n",
    "nombre_archivo = \"mi_reward.py\"\n",
    "\n",
    "# Guardar el contenido en el archivo .py\n",
    "with open(nombre_archivo, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(codigo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332cb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
