{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8818dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31154b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c11d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rl-train.py', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0318fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import gymnasium as gym\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "\n",
      "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
      "print(f\"Using {device} device\")\n",
      "\n",
      "\n",
      "class PolicyNet(nn.Module):\n",
      "    def __init__(self, obs_dim, act_dim):\n",
      "        super().__init__()\n",
      "        self.stack = nn.Sequential(\n",
      "            nn.Linear(obs_dim, 64),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(64, act_dim)\n",
      "        )\n",
      "    \n",
      "    def forward(self, x):\n",
      "        return self.stack(x)\n",
      "    \n",
      "\n",
      "def get_new_reward(obs):\n",
      "    reward = 1\n",
      "    return reward\n",
      "\n",
      "env = gym.make(\"CartPole-v1\")\n",
      "\n",
      "obs_dim = env.observation_space.shape[0]\n",
      "act_dim = env.action_space.n\n",
      "\n",
      "model_policy = PolicyNet(obs_dim, act_dim)\n",
      "\n",
      "optimizer = torch.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
      "\n",
      "for episode in range(4000):\n",
      "    obs, _ = env.reset()\n",
      "    log_probs = []\n",
      "    rewards = []\n",
      "\n",
      "    done = False\n",
      "    while not done:\n",
      "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
      "        logits = model_policy(obs_tensor)\n",
      "        probs = torch.softmax(logits, dim=-1)\n",
      "        dist = torch.distributions.Categorical(probs)\n",
      "        action = dist.sample()\n",
      "\n",
      "        log_probs.append(dist.log_prob(action))\n",
      "\n",
      "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
      "\n",
      "        reward = get_new_reward(obs)\n",
      "\n",
      "        done = terminated or truncated\n",
      "        rewards.append(reward)\n",
      "\n",
      "    # Calcular retorno total (suma de rewards)\n",
      "    total_return = sum(rewards)\n",
      "\n",
      "    # Actualizar policy (REINFORCE)\n",
      "    loss = []\n",
      "    for log_prob in log_probs:\n",
      "        loss.append(-log_prob * total_return)\n",
      "    loss = torch.stack(loss).sum()\n",
      "\n",
      "    optimizer.zero_grad()\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    print(f\"Episode {episode}, Return: {total_return}\")\n",
      "\n",
      "env.close()\n",
      "\n",
      "# Guardar modelo entrenado\n",
      "torch.save(model_policy.state_dict(), \"policy_cartpole_frompy.pt\")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ddc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = \"\"\"\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\"\"\"\n",
    "\n",
    "description_problem = \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\"\"\"\n",
    "\n",
    "episode_end = \"\"\"\n",
    "## Episode End\n",
    "\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59abad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tengo un problema de reinforcement learning definido como:\n",
    "\n",
    "{description_problem}\n",
    "\n",
    "para dicho problema, lo resolvimos usando el siguiente codigo de reinforcement learning:\n",
    "\n",
    "{data}\n",
    "\n",
    "quiero que modifiques la funcion llamada get_new_reward y que me des la nueva funcion. \n",
    "Esta funcion recibe como parametro la variable obs, que está definida como sigue:\n",
    "\n",
    "{observation_space}\n",
    "\n",
    "recuerda que el episodio termina con las siguientes caracteristicas, el caso de truncation, es un caso de éxito del problema:\n",
    "\n",
    "{episode_end}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9caba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tengo un problema de reinforcement learning definido como:\n",
      "\n",
      "\n",
      "    ## Description\n",
      "\n",
      "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
      "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
      "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
      "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
      "     in the left and right direction on the cart.\n",
      "\n",
      "\n",
      "para dicho problema, lo resolvimos usando el siguiente codigo de reinforcement learning:\n",
      "\n",
      "import random\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import gymnasium as gym\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "\n",
      "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
      "print(f\"Using {device} device\")\n",
      "\n",
      "\n",
      "class PolicyNet(nn.Module):\n",
      "    def __init__(self, obs_dim, act_dim):\n",
      "        super().__init__()\n",
      "        self.stack = nn.Sequential(\n",
      "            nn.Linear(obs_dim, 64),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(64, act_dim)\n",
      "        )\n",
      "    \n",
      "    def forward(self, x):\n",
      "        return self.stack(x)\n",
      "    \n",
      "\n",
      "def get_new_reward(obs):\n",
      "    reward = 1\n",
      "    return reward\n",
      "\n",
      "env = gym.make(\"CartPole-v1\")\n",
      "\n",
      "obs_dim = env.observation_space.shape[0]\n",
      "act_dim = env.action_space.n\n",
      "\n",
      "model_policy = PolicyNet(obs_dim, act_dim)\n",
      "\n",
      "optimizer = torch.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
      "\n",
      "for episode in range(4000):\n",
      "    obs, _ = env.reset()\n",
      "    log_probs = []\n",
      "    rewards = []\n",
      "\n",
      "    done = False\n",
      "    while not done:\n",
      "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
      "        logits = model_policy(obs_tensor)\n",
      "        probs = torch.softmax(logits, dim=-1)\n",
      "        dist = torch.distributions.Categorical(probs)\n",
      "        action = dist.sample()\n",
      "\n",
      "        log_probs.append(dist.log_prob(action))\n",
      "\n",
      "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
      "\n",
      "        reward = get_new_reward(obs)\n",
      "\n",
      "        done = terminated or truncated\n",
      "        rewards.append(reward)\n",
      "\n",
      "    # Calcular retorno total (suma de rewards)\n",
      "    total_return = sum(rewards)\n",
      "\n",
      "    # Actualizar policy (REINFORCE)\n",
      "    loss = []\n",
      "    for log_prob in log_probs:\n",
      "        loss.append(-log_prob * total_return)\n",
      "    loss = torch.stack(loss).sum()\n",
      "\n",
      "    optimizer.zero_grad()\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    print(f\"Episode {episode}, Return: {total_return}\")\n",
      "\n",
      "env.close()\n",
      "\n",
      "# Guardar modelo entrenado\n",
      "torch.save(model_policy.state_dict(), \"policy_cartpole_frompy.pt\")\n",
      "\n",
      "quiero que modifiques la funcion llamada get_new_reward y que me des la nueva funcion. \n",
      "Esta funcion recibe como parametro la variable obs, que está definida como sigue:\n",
      "\n",
      "\n",
      "    ## Observation Space\n",
      "\n",
      "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
      "\n",
      "    | Num | Observation           | Min                 | Max               |\n",
      "    |-----|-----------------------|---------------------|-------------------|\n",
      "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
      "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
      "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
      "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
      "\n",
      "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
      "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
      "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
      "       if the cart leaves the `(-2.4, 2.4)` range.\n",
      "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
      "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
      "\n",
      "\n",
      "recuerda que el episodio termina con las siguientes caracteristicas, el caso de truncation, es un caso de éxito del problema:\n",
      "\n",
      "\n",
      "## Episode End\n",
      "\n",
      "    The episode ends if any one of the following occurs:\n",
      "\n",
      "    1. Termination: Pole Angle is greater than ±12°\n",
      "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
      "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b523fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", \n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=[\n",
    "            \"You are a Reinforcement Learning Reward Function Design Specialist.\",\n",
    "            \"Your mission is to help design, analyze, and improve reward functions for reinforcement learning agents across various environments and tasks.\",\n",
    "            \"You are highly skilled in identifying sparse, deceptive, or poorly shaped rewards and proposing alternatives that improve learning efficiency and agent behavior.\",\n",
    "            \"You understand how to align reward signals with long-term objectives, avoid reward hacking, and incorporate domain knowledge into reward shaping.\",\n",
    "            \"You are familiar with a wide range of RL algorithms such as PPO, DDPG, SAC, A3C, Q-learning, and understand how reward design interacts with each algorithm's learning dynamics.\",\n",
    "            \"You always respond with code only — no explanations, no comments, no additional text — just the raw code necessary to implement the reward function or solution requested.\",\n",
    "            \"Always ask clarifying questions if the problem is underspecified or if the task goals are ambiguous.\"\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    ),   \n",
    "    contents=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "879e9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def get_new_reward(obs):\n",
      "    cart_position, cart_velocity, pole_angle, pole_velocity = obs\n",
      "    \n",
      "    # Penalize for being close to the termination boundaries\n",
      "    # Cart position penalty\n",
      "    cart_pos_penalty = 0\n",
      "    if abs(cart_position) > 2.0:  # Closer to the edge\n",
      "        cart_pos_penalty = (abs(cart_position) - 2.0) * 0.5 \n",
      "        \n",
      "    # Pole angle penalty\n",
      "    pole_angle_penalty = 0\n",
      "    if abs(pole_angle) > 0.15: # Closer to the termination angle\n",
      "        pole_angle_penalty = (abs(pole_angle) - 0.15) * 1.0\n",
      "\n",
      "    # Penalize for high velocities (less stable)\n",
      "    velocity_penalty = abs(cart_velocity) * 0.1 + abs(pole_velocity) * 0.2\n",
      "\n",
      "    # Base reward for staying alive\n",
      "    reward = 1.0 - cart_pos_penalty - pole_angle_penalty - velocity_penalty\n",
      "    \n",
      "    # Ensure reward is not negative, though the penalties are designed to be small\n",
      "    reward = max(0, reward) \n",
      "    \n",
      "    return reward\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d166f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = response.text.split('\\n')[1:-1]\n",
    "tmp = response.text.splitlines()[1:-1]\n",
    "codigo = \"\\n\".join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8448bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_new_reward(obs):\n",
      "    cart_position, cart_velocity, pole_angle, pole_velocity = obs\n",
      "    \n",
      "    # Penalize for being close to the termination boundaries\n",
      "    # Cart position penalty\n",
      "    cart_pos_penalty = 0\n",
      "    if abs(cart_position) > 2.0:  # Closer to the edge\n",
      "        cart_pos_penalty = (abs(cart_position) - 2.0) * 0.5 \n",
      "        \n",
      "    # Pole angle penalty\n",
      "    pole_angle_penalty = 0\n",
      "    if abs(pole_angle) > 0.15: # Closer to the termination angle\n",
      "        pole_angle_penalty = (abs(pole_angle) - 0.15) * 1.0\n",
      "\n",
      "    # Penalize for high velocities (less stable)\n",
      "    velocity_penalty = abs(cart_velocity) * 0.1 + abs(pole_velocity) * 0.2\n",
      "\n",
      "    # Base reward for staying alive\n",
      "    reward = 1.0 - cart_pos_penalty - pole_angle_penalty - velocity_penalty\n",
      "    \n",
      "    # Ensure reward is not negative, though the penalties are designed to be small\n",
      "    reward = max(0, reward) \n",
      "    \n",
      "    return reward\n"
     ]
    }
   ],
   "source": [
    "print(codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41131d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nombre del archivo\n",
    "nombre_archivo = \"mi_reward.py\"\n",
    "\n",
    "# Guardar el contenido en el archivo .py\n",
    "with open(nombre_archivo, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(codigo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332cb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
